{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Transformer\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jeonghee Jo, October 2019, and modified by Jungbeom Lee, October 2020.\n",
    "\n",
    "This is about Transformer (Vaswani et al., 2017).\n",
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/Kyubyong/transformer (Tensorflow)\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html (PyTorch)\n",
    "\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of Transformers more clearly in a code level.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* 20xx-xxxxx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Grading is as follows:\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. Train your model using at least <font color=red> 12 different hyperparameter set </font>. Report performance results computed in the last code block <font color=red> for corresponding each hyperparameter set </font>. Plus, <font color=red> submit the one checkpoint file </font> of your best model. \n",
    "\n",
    "2. Please provide the analysis of changed hyper-parameters. (10 points)\n",
    "\n",
    "The details are described in <font color=red>**transformer_modules.py**</font>. (There is nothing to implement in this notebook.)\n",
    "\n",
    "\n",
    "Now proceed to the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change hyper-parameters in this code block!\n",
    "import random\n",
    "\n",
    "emsize = random.randint(200,600) # embedding dimension\n",
    "nhid = random.randint(200,600) # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = random.randint(2,7) # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = random.randint(2,7) # the number of heads in the multiheadattention models\n",
    "emsize = int(emsize/nhead)*nhead # for dividable\n",
    "dropout = random.uniform(0.1,0.5) # the dropout value\n",
    "batch_size = 128\n",
    "eval_batch_size = 128\n",
    "epochs = 3 # The number of epochs\n",
    "\n",
    "# For Best Model\n",
    "emsize = 416\n",
    "nhead = 2\n",
    "nhid = 399\n",
    "nlayers = 2\n",
    "dropout = 0.11775754439467333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import *\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaekyungcho/anaconda3/envs/deep-learning-20/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:350: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  465 batches | lr 5.00 | ms/batch 76.70 | loss  8.48 | ppl  4826.58\n",
      "| epoch   1 |   400/  465 batches | lr 5.00 | ms/batch 75.47 | loss  6.73 | ppl   839.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 36.50s | valid loss  6.10 | valid ppl   444.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  465 batches | lr 4.51 | ms/batch 76.22 | loss  6.15 | ppl   468.81\n",
      "| epoch   2 |   400/  465 batches | lr 4.51 | ms/batch 76.11 | loss  5.93 | ppl   374.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 36.58s | valid loss  5.70 | valid ppl   297.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  465 batches | lr 4.29 | ms/batch 76.64 | loss  5.71 | ppl   301.15\n",
      "| epoch   3 |   400/  465 batches | lr 4.29 | ms/batch 76.58 | loss  5.58 | ppl   266.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 36.78s | valid loss  5.49 | valid ppl   243.21\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "torch.save(best_model.state_dict(), 'models/Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "\n",
      "| End of training | test loss  5.42 | test ppl   225.02  \n",
      "| Hyperparam Set | emsize 416 | nhead 2 | nhid 399 | nlayers 2 | dropout 0.11775754439467333  \n",
      "\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_model_loaded = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "best_model_loaded.load_state_dict(torch.load('models/Transformer.pth'), strict=True)\n",
    "test_loss = evaluate(best_model_loaded, test_data)\n",
    "print('=' * 89)\n",
    "print('')\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}  '.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('| Hyperparam Set | emsize {} | nhead {} | nhid {} | nlayers {} | dropout {}  \\n'.format(\n",
    "    emsize, nhead, nhid, nlayers, dropout))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please provide your analysis on each hyper-parameter.\n",
    "\n",
    "총 12개의 hyperparameter set에 대하여 test loss 와 test ppl을 구하여 나타내었다.\n",
    "\n",
    "hyperparameter의 경우 random함수를 사용하여 sampling 하였다. emsize가 nhead로 나누어 떨어져야하기 때문에 그 부분을 조금 조정한 것 이외에는 random 함수에 의존하여 sampling 하였다. sampling 범위의 경우 상식적인 선에서 임의로 결정하였다.\n",
    "\n",
    "그 결과는 아래와 같다.\n",
    "\n",
    "========Set 1===========================================================================\n",
    "\n",
    "| End of training | test loss  5.55 | test ppl   257.36  \n",
    "| Hyperparam Set | emsize 200 | nhead 2 | nhid 200 | nlayers 2 | dropout 0.2  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 2============================================================================\n",
    "\n",
    "| End of training | test loss  5.61 | test ppl   273.16  \n",
    "| Hyperparam Set | emsize 400 | nhead 2 | nhid 436 | nlayers 4 | dropout 0.42629237581765866  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 3============================================================================\n",
    "\n",
    "| End of training | test loss  5.66 | test ppl   285.89  \n",
    "| Hyperparam Set | emsize 484 | nhead 4 | nhid 337 | nlayers 4 | dropout 0.3585654657685182  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 4============================================================================\n",
    "\n",
    "| End of training | test loss  5.73 | test ppl   307.61  \n",
    "| Hyperparam Set | emsize 412 | nhead 4 | nhid 404 | nlayers 3 | dropout 0.4923271460494234  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 5============================================================================\n",
    "\n",
    "| End of training | test loss  5.66 | test ppl   286.79  \n",
    "| Hyperparam Set | emsize 478 | nhead 2 | nhid 435 | nlayers 2 | dropout 0.4453517332554101  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 6============================================================================\n",
    "\n",
    "| End of training | test loss  5.55 | test ppl   257.85  \n",
    "| Hyperparam Set | emsize 280 | nhead 4 | nhid 527 | nlayers 3 | dropout 0.33495872352126155  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 7============================================================================\n",
    "\n",
    "| End of training | test loss  5.48 | test ppl   239.25  \n",
    "| Hyperparam Set | emsize 362 | nhead 2 | nhid 497 | nlayers 3 | dropout 0.27982980144127834  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 8============================================================================\n",
    "\n",
    "| End of training | test loss  5.38 | test ppl   216.49  \n",
    "| Hyperparam Set | emsize 416 | nhead 2 | nhid 399 | nlayers 2 | dropout 0.11775754439467333  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 9============================================================================\n",
    "\n",
    "| End of training | test loss  6.67 | test ppl   788.14  \n",
    "| Hyperparam Set | emsize 588 | nhead 6 | nhid 287 | nlayers 7 | dropout 0.21496531402778696  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 10===========================================================================\n",
    "\n",
    "| End of training | test loss  5.42 | test ppl   225.32  \n",
    "| Hyperparam Set | emsize 504 | nhead 7 | nhid 511 | nlayers 3 | dropout 0.1708669080824745  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 11===========================================================================\n",
    "\n",
    "| End of training | test loss  5.79 | test ppl   327.94  \n",
    "| Hyperparam Set | emsize 376 | nhead 4 | nhid 516 | nlayers 5 | dropout 0.47521358575208905  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "========Set 12===========================================================================\n",
    "\n",
    "| End of training | test loss  5.74 | test ppl   312.20  \n",
    "| Hyperparam Set | emsize 366 | nhead 2 | nhid 428 | nlayers 3 | dropout 0.4774593507973922  \n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "간략하게 결과를 살펴보면, dropout 이 작을 수록 test loss가 작다는 것을 알 수 있다. \n",
    "이는 epoch가 충분히 길지 않기 때문에, dropout이 작을수록 초반 학습이 빨라 이러한 결과가 도출되었을 가능성이 있다.\n",
    "\n",
    "또한 nlayers가 너무 크거나, nhid가 emsize에 비해 작을 때에도 좋지 않을 결과가 도출됨을 알 수 있었다.\n",
    "이 또한 충분하지 않은 epoch로 인해 deep layer일 수록 초반 학습이 느려 이러한 결과가 도출되었을 수 있다.\n",
    "\n",
    "최종적으로 가장 test loss 가 작은 hyperparameter set은 8번 set으로 \n",
    "| Hyperparam Set | emsize 416 | nhead 2 | nhid 399 | nlayers 2 | dropout 0.11775754439467333 \n",
    "의 hyperparameter set을 가진다. 적당히 큰 embedding size와 hidden dimension, 그리고 낮은 dropout, 상대적으로 얇은 네트워크 depth로 인해 초기 학습이 빠르게 진행되었다고 판단된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-20] *",
   "language": "python",
   "name": "conda-env-deep-learning-20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 2: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sangil Lee, October 2018, modified by Jungbeom Lee, October 2020.\n",
    "\n",
    "This is  a basic character-level RNN to classify words.\n",
    "\n",
    "A character-level RNN reads words as a series of characters - outputting a prediction and ?hidden state? at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we will train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
    "\n",
    "\n",
    "Original blog post & code:\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Classifying words with character-level RNN (30 points)\n",
    "\n",
    "\n",
    "1. Successful training through implementing code that works. You will need to implement the codes in char_rnn.py.  (15 points)\n",
    "\n",
    "\n",
    "2. After training, the final accuracy must be <font color=red> above 65% </font> (please see the last code block). We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the outputs of all the code blocks! (15 points)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Japanese.txt', 'data/names/Russian.txt', 'data/names/Scottish.txt', 'data/names/Chinese.txt', 'data/names/Korean.txt', 'data/names/Czech.txt', 'data/names/French.txt', 'data/names/Italian.txt', 'data/names/Spanish.txt', 'data/names/Greek.txt', 'data/names/Vietnamese.txt', 'data/names/Dutch.txt', 'data/names/Portuguese.txt', 'data/names/English.txt', 'data/names/Arabic.txt', 'data/names/Irish.txt', 'data/names/German.txt', 'data/names/Polish.txt']\n",
      "lusarski\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('lus√†rski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names to Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for training and inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters, should not be changed.\n",
    "N_CATEGORIES = len(all_categories)\n",
    "N_LETTERS = len(all_letters)\n",
    "N_EVAL_SAMPLES_PER_CATEGORY = 10\n",
    "\n",
    "# Adjustable parameters. You can change these parameters freely.\n",
    "\n",
    "N_HIDDEN = 128\n",
    "LEARNING_RATE = 8e-3\n",
    "N_ITERS = 300000\n",
    "print_every = 5000\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Russian / line = Davydovich\n",
      "category = Polish / line = Wojewodka\n",
      "category = Portuguese / line = Duarte\n",
      "category = Italian / line = Colombo\n",
      "category = Korean / line = Yim\n",
      "category = Dutch / line = Donk\n",
      "category = Italian / line = Nascimbene\n",
      "category = English / line = Gaudin\n",
      "category = Czech / line = Entler\n",
      "category = Czech / line = Novy Novy\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def load_train_example(n_per_cls):\n",
    "    category_tensors, line_tensors = [], []\n",
    "    for category in all_categories:\n",
    "        for line in category_lines[category][:n_per_cls]:\n",
    "            category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "            line_tensor = lineToTensor(line)\n",
    "            category_tensors.append(category_tensor)\n",
    "            line_tensors.append(line_tensor)\n",
    "    \n",
    "    return category_tensors, line_tensors\n",
    "    \n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "category_tensors, line_tensors = load_train_example(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should NOT change this code block\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    category_tensor = category_tensor.cuda()\n",
    "    line_tensor = line_tensor.cuda()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-LEARNING_RATE)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "5000 1% (2.606928372579813) Kassis / Greek ? (Arabic) 0.008\n",
      "10000 3% (2.1115797169730066) Anetakis / Greek correct! 0.0076\n",
      "15000 5% (1.895802791620791) Youn / Chinese ? (Korean) 0.00722\n",
      "20000 6% (1.7773084564601072) Tomioka / Japanese correct! 0.006859\n",
      "25000 8% (1.650728056069836) Kassmeyer / Dutch ? (German) 0.006516049999999999\n",
      "30000 10% (1.5909515379429795) Benedetti / Italian correct! 0.006190247499999998\n",
      "35000 11% (1.5437998853828292) Lin / Chinese correct! 0.005880735124999998\n",
      "40000 13% (1.499422970969649) Kent / Chinese ? (English) 0.005586698368749998\n",
      "45000 15% (1.4517796252121726) Fonseca / Spanish ? (Portuguese) 0.005307363450312497\n",
      "50000 16% (1.3989667176824063) Peeters / Portuguese ? (Dutch) 0.005041995277796872\n",
      "55000 18% (1.3725967236567638) Chicken / Dutch ? (Czech) 0.004789895513907028\n",
      "60000 20% (1.3136802208601845) Mcneil / Scottish ? (Irish) 0.004550400738211676\n",
      "65000 21% (1.333817897288729) Schermer / German ? (Dutch) 0.004322880701301092\n",
      "70000 23% (1.2961872665233096) Essert / Dutch ? (German) 0.004106736666236037\n",
      "75000 25% (1.3066603410875746) Zhao / Chinese correct! 0.0039013998329242355\n",
      "80000 26% (1.26557294240568) Gensch / Scottish ? (German) 0.0037063298412780238\n",
      "85000 28% (1.2812912328952808) Kitabatake / Japanese correct! 0.0035210133492141224\n",
      "90000 30% (1.231718558764836) Asghar / Arabic correct! 0.0033449626817534162\n",
      "95000 31% (1.2306756802447403) Oh  / Korean correct! 0.003177714547665745\n",
      "100000 33% (1.2146657207083575) Naifeh / Arabic correct! 0.0030188288202824577\n",
      "105000 35% (1.1829105789608672) Hyun  / Korean correct! 0.0028678873792683346\n",
      "110000 36% (1.151155257983779) Okui / Japanese correct! 0.0027244930103049176\n",
      "115000 38% (1.1759238151143683) Guketlev / Russian correct! 0.0025882683597896715\n",
      "120000 40% (1.15607494004756) Napoletani / Italian correct! 0.0024588549418001877\n",
      "125000 41% (1.1458679222138366) Kwong / Chinese correct! 0.002335912194710178\n",
      "130000 43% (1.1184271026722679) So / Korean correct! 0.002219116584974669\n",
      "135000 45% (1.0970484650122883) Mathieu / Japanese ? (French) 0.0021081607557259354\n",
      "140000 46% (1.1306820137791649) Rumpade / Dutch correct! 0.0020027527179396383\n",
      "145000 48% (1.0685589332504715) Rhodes / Dutch ? (English) 0.0019026150820426564\n",
      "150000 50% (1.0909490650757752) Rocchi / Italian correct! 0.0018074843279405235\n",
      "155000 51% (1.0586200233589713) Minabuchi / Italian ? (Japanese) 0.0017171101115434973\n",
      "160000 53% (1.0628206260704973) Lim  / Korean correct! 0.0016312546059663224\n",
      "165000 55% (1.0577245864320655) Seo / Korean correct! 0.0015496918756680062\n",
      "170000 56% (1.0559232802165084) Holodny / English ? (Russian) 0.0014722072818846058\n",
      "175000 58% (1.032257083968923) Vaccaro / Italian correct! 0.0013985969177903754\n",
      "180000 60% (1.0264003232209833) Mayer / German ? (Czech) 0.0013286670719008565\n",
      "185000 61% (1.024411284688194) Muir / German ? (Scottish) 0.0012622337183058137\n",
      "190000 63% (1.054818397072007) Mansour / Arabic correct! 0.001199122032390523\n",
      "195000 65% (1.0197334207062514) Grimaldi / Italian correct! 0.0011391659307709967\n",
      "200000 66% (1.0134571478733307) Hierro / Spanish correct! 0.0010822076342324467\n",
      "205000 68% (1.0182915609509164) Abrami / Italian correct! 0.0010280972525208243\n",
      "210000 70% (1.0021113275872653) Sinclair / Scottish correct! 0.000976692389894783\n",
      "215000 71% (0.988761957615866) Ha / Vietnamese correct! 0.0009278577704000438\n",
      "220000 73% (0.9744713498727069) Hakimi / Japanese ? (Arabic) 0.0008814648818800415\n",
      "225000 75% (0.9760279504484847) Tykal / Czech correct! 0.0008373916377860394\n",
      "230000 76% (0.9916087247356394) Noh / Korean correct! 0.0007955220558967374\n",
      "235000 78% (0.9696779628241462) Jedynak / Czech ? (Polish) 0.0007557459531019005\n",
      "240000 80% (0.948687825180261) Hobson / Scottish ? (English) 0.0007179586554468054\n",
      "245000 81% (0.9727241316278443) Mei / Chinese correct! 0.0006820607226744651\n",
      "250000 83% (0.9379391764896711) Hagan / Arabic ? (English) 0.0006479576865407419\n",
      "255000 85% (0.9485195756573106) Strand / German correct! 0.0006155598022137048\n",
      "260000 86% (0.9418919911369193) Adelhanoff / Russian correct! 0.0005847818121030195\n",
      "265000 88% (0.9664269805059951) Raimov / Russian correct! 0.0005555427214978685\n",
      "270000 90% (0.942793392490595) Kedzierski / Polish correct! 0.0005277655854229751\n",
      "275000 91% (0.9395060215633224) Edgerton / English correct! 0.0005013773061518263\n",
      "280000 93% (0.9695311653998556) Mcgregor / Scottish correct! 0.00047630844084423497\n",
      "285000 95% (0.9371663833977311) Auer / German correct! 0.0004524930188020232\n",
      "290000 96% (0.953286668915016) Mansour / Arabic correct! 0.000429868367861922\n",
      "295000 98% (0.938027841442782) Ly / Vietnamese correct! 0.00040837494946882586\n",
      "300000 100% (0.9723121751184924) Del bosque / Spanish correct! 0.00038795620199538457\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from char_rnn import RNN\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_CATEGORIES).cuda()\n",
    "rnn.train()\n",
    "criterion = nn.NLLLoss()\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "for iter in range(1, N_ITERS + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = 'correct!' if guess == category else '? (%s)' % category\n",
    "        print('%d %d%% (%s) %s / %s %s %s' % (iter, iter / N_ITERS * 100, current_loss/print_every, line, guess, correct, LEARNING_RATE))\n",
    "        current_loss = 0\n",
    "        LEARNING_RATE *= 0.95\n",
    "torch.save(rnn.state_dict(), 'models/RNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval with 180 samples\n",
      "1.0285803009298333\n",
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "## Should NOT change this code block\n",
    "category_tensors, line_tensors = load_train_example(N_EVAL_SAMPLES_PER_CATEGORY)\n",
    "\n",
    "total_loss = 0\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "for idx in range(len(category_tensors)):\n",
    "    n_samples += 1\n",
    "    category_tensor, line_tensor = category_tensors[idx].cuda(), line_tensors[idx].cuda()\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     print(guess_i, category_tensor)\n",
    "    if guess_i == category_tensor[0].data.cpu().numpy():\n",
    "        n_correct += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "print(\"eval with %d samples\" % n_samples)\n",
    "print(total_loss / n_samples)\n",
    "print(n_correct / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval with 180 samples\n",
      "1.0285803009298333\n",
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "# check your saved checkpoint again\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_CATEGORIES).cuda()\n",
    "rnn.eval()\n",
    "rnn.load_state_dict(torch.load('models/RNN.pth'), strict=True)\n",
    "\n",
    "category_tensors, line_tensors = load_train_example(N_EVAL_SAMPLES_PER_CATEGORY)\n",
    "\n",
    "total_loss = 0\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "for idx in range(len(category_tensors)):\n",
    "    n_samples += 1\n",
    "    category_tensor, line_tensor = category_tensors[idx].cuda(), line_tensors[idx].cuda()\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     print(guess_i, category_tensor)\n",
    "    if guess_i == category_tensor[0].data.cpu().numpy():\n",
    "        n_correct += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "print(\"eval with %d samples\" % n_samples)\n",
    "print(total_loss / n_samples)\n",
    "print(n_correct / n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-20] *",
   "language": "python",
   "name": "conda-env-deep-learning-20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

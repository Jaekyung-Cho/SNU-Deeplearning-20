{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #1 Part 2: Implementing Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Previously in `Assignment1-1_Data_Curation.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to implement a simple 3-layer neural network from scratch. We won't derive all the math that's required, but I will try to give an intuitive explanation of what we are doing and will point to resources to read up on the details.\n",
    "\n",
    "But why implement a Neural Network from scratch at all? Even if you plan on using Neural Network libraries like [PyBrain](http://pybrain.org) in the future, implementing a network from scratch at least once is an extremely valuable exercise. It helps you gain an understanding of how neural networks work, and that is essential to designing effective models.\n",
    "\n",
    "One thing to note is that the code examples here aren't terribly efficient. They are meant to be easy to understand. In an upcoming part of the assignment, we will explore how to write an efficient Neural Network implementation using [PyTorch](http://pytorch.org/). \n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **part 1 - 3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a compressed file called *[Your student number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; 20\\*\\*-\\*\\*\\*\\*\\*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "\n",
    "First reload the data we generated in `Assignment2-1_Data_Curation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/home/jackyoung96/2020_2/Deeplearning_assignment/HW1_data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- unnormalize data\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset * 255.0 + 255.0/2\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # one-hot encoding, Map the label 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n"
     ]
    }
   ],
   "source": [
    "data_size = 2000\n",
    "train_dataset = train_dataset[0:data_size]\n",
    "train_labels = train_labels[0:data_size]\n",
    "\n",
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample network](Utils/nn-from-scratch-3-layer-network-1024x693.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of our data, 784. Similarly, the number of nodes in the output layer is determined by the number of classes we have, 10. The input to the network will be the pixel values of the input image and its output will be ten probabilities, ones for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How our network makes predictions\n",
    "\n",
    "Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) we defined above. If $x$ is the 784-dimensional input to our network then we calculate our prediction $\\hat{y}$ (ten-dimensional) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 & = xW_1 + b_1 \\\\\n",
    "a_1 & = \\tanh(z_1) \\\\\n",
    "z_2 & = a_1W_2 + b_2 \\\\\n",
    "a_2 & = \\hat{y} = \\mathrm{softmax}(z_2)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_i$ is the input of layer $i$ and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we need to learn from our training data. You can think of them as matrices transforming data between layers of the network. Looking at the matrix multiplications above we can figure out the dimensionality of these matrices. If we use 1024 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{784\\times1024}$, $b_1 \\in \\mathbb{R}^{1024}$, $W_2 \\in \\mathbb{R}^{1024\\times10}$, $b_2 \\in \\mathbb{R}^{10}$. Now you see why we have more parameters if we increase the size of the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the Parameters\n",
    "\n",
    "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our goal is to find the parameters that minimize our loss function. We can use [gradient descent](http://cs231n.github.io/optimization-1/) to find its minimum. I will implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice. So if you are serious you'll want to use one of these, and ideally you would also [decay the learning rate over time](http://cs231n.github.io/neural-networks-3/#anneal).\n",
    "\n",
    "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output. I won't go into detail how backpropagation works, but there are many excellent explanations ([here](http://colah.github.io/posts/2015-08-Backprop/) or [here](http://cs231n.github.io/optimization-2/)) floating around the web.\n",
    "\n",
    "Applying the backpropagation formula we find the following (trust me on this):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "& \\delta_3 = \\hat{y} - y \\\\\n",
    "& \\delta_2 = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_3\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{W_1}} = x^T \\delta_2\\\\\n",
    "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations functions\n",
    "\n",
    "There are various activation functions in neural networks. \n",
    "According to the characteristics of each activation function, the type of the neural network, and the data type, appropriate activation functions are used.\n",
    "\n",
    "![Activation Functions](Utils/activation-functions.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now we are ready for our implementation. We start by defining some useful variables and parameters for gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "num_examples = len(train_dataset) # training set size\n",
    "print(num_examples)\n",
    "nn_input_dim = 784 # input layer dimensionality\n",
    "nn_output_dim = 10 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "First let's implement the loss function we defined above. We use this to evaluate how well our model is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate the total loss on the dataset\n",
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = train_dataset.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log([probs[i,np.nonzero(train_labels)[(1)][i].astype('int64')] for i in range(num_examples)])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement a helper function to calculate the output of the network. It does forward propagation as defined above and returns the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "Finally, here comes the function to train our Neural Network. It implements batch gradient descent using the backpropagation derivates we found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = train_dataset.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = (probs - train_labels) / data_size\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(train_dataset.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, calculate_loss(model)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A network with a hidden layer of size 10\n",
    "\n",
    "Let's see what happens if we train a network with a hidden layer size of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.409069\n",
      "Loss after iteration 1000: 0.951440\n",
      "Loss after iteration 2000: 0.737469\n",
      "Loss after iteration 3000: 0.639097\n",
      "Loss after iteration 4000: 0.577911\n",
      "Loss after iteration 5000: 0.535709\n",
      "Loss after iteration 6000: 0.504747\n",
      "Loss after iteration 7000: 0.480976\n",
      "Loss after iteration 8000: 0.462012\n",
      "Loss after iteration 9000: 0.446527\n",
      "Loss after iteration 10000: 0.433805\n",
      "Loss after iteration 11000: 0.423181\n",
      "Loss after iteration 12000: 0.414191\n",
      "Loss after iteration 13000: 0.406507\n",
      "Loss after iteration 14000: 0.399886\n",
      "Loss after iteration 15000: 0.394142\n",
      "Loss after iteration 16000: 0.389138\n",
      "Loss after iteration 17000: 0.384766\n",
      "Loss after iteration 18000: 0.380936\n",
      "Loss after iteration 19000: 0.377566\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a 10-dimensional hidden layer\n",
    "model = build_model(10, print_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the hidden layer size\n",
    "\n",
    "In the example above we picked a hidden layer size of 10. Let's now get a sense of how varying the hidden layer size affects the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.330436\n",
      "Loss after iteration 1000: 0.666129\n",
      "Loss after iteration 2000: 0.560475\n",
      "Loss after iteration 3000: 0.501706\n",
      "Loss after iteration 4000: 0.459902\n",
      "Loss after iteration 5000: 0.427561\n",
      "Loss after iteration 6000: 0.401522\n",
      "Loss after iteration 7000: 0.380165\n",
      "Loss after iteration 8000: 0.362433\n",
      "Loss after iteration 9000: 0.347567\n",
      "Loss after iteration 10000: 0.334993\n",
      "Loss after iteration 11000: 0.324275\n",
      "Loss after iteration 12000: 0.315080\n",
      "Loss after iteration 13000: 0.307149\n",
      "Loss after iteration 14000: 0.300276\n",
      "Loss after iteration 15000: 0.294291\n",
      "Loss after iteration 16000: 0.289057\n",
      "Loss after iteration 17000: 0.284456\n",
      "Loss after iteration 18000: 0.280395\n",
      "Loss after iteration 19000: 0.276795\n",
      "Loss after iteration 0: 2.362255\n",
      "Loss after iteration 1000: 0.650155\n",
      "Loss after iteration 2000: 0.551860\n",
      "Loss after iteration 3000: 0.495918\n",
      "Loss after iteration 4000: 0.455431\n",
      "Loss after iteration 5000: 0.423593\n",
      "Loss after iteration 6000: 0.397577\n",
      "Loss after iteration 7000: 0.375869\n",
      "Loss after iteration 8000: 0.357527\n",
      "Loss after iteration 9000: 0.341900\n",
      "Loss after iteration 10000: 0.328507\n",
      "Loss after iteration 11000: 0.316978\n",
      "Loss after iteration 12000: 0.307025\n",
      "Loss after iteration 13000: 0.298410\n",
      "Loss after iteration 14000: 0.290936\n",
      "Loss after iteration 15000: 0.284436\n",
      "Loss after iteration 16000: 0.278768\n",
      "Loss after iteration 17000: 0.273813\n",
      "Loss after iteration 18000: 0.269474\n",
      "Loss after iteration 19000: 0.265664\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_dimensions = [50, 100]\n",
    "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
    "    model = build_model(nn_hdim, print_loss=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while a hidden layer of low dimensionality nicely capture the general trend of our data, but higher dimensionalities are prone to overfitting. They are \"memorizing\" the data as opposed to fitting the general shape. If we were to evaluate our model on a separate test set (and you should!) the model with a smaller hidden layer size would likely perform better because it generalizes better. We could counteract overfitting with stronger regularization, but picking the a correct size for hidden layer is a much more \"economical\" solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2\n",
    "\n",
    "Implement neural network with a <font color='red'>$two\\ hidden\\ layer$</font> to improve your model's validation / test accuracy as much as you can. You just can copy and paste the code above, but since relevant materials can appear on the exam, I strongly recommend you to implement it yourself.\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. Instead of batch gradient descent, use **minibatch** gradient descent ([more info](http://cs231n.github.io/optimization-1/#gd)) to train the network. Minibatch gradient descent typically performs better in practice. \n",
    "2. We used a fixed learning rate epsilon for gradient descent. Implement an **annealing** schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). \n",
    "3. We used a tanh activation function for our hidden layer. Experiment with other activation functions such as **ReLU** function. Note that changing the activation function also means changing the backpropagation derivative.\n",
    "\n",
    "**Evaluation**: Use print_loss option and show the model actually train. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\"\"\" TODO \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pickle_file = '/home/jackyoung96/2020_2/Deeplearning_assignment/HW1_data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset * 255.0 + 255.0/2\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # one-hot encoding, Map the label 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasize and hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "data_size = 2000\n",
    "valid_size = 200\n",
    "\n",
    "train_dataset = train_dataset[0:data_size]\n",
    "train_labels = train_labels[0:data_size]\n",
    "valid_dataset = valid_dataset[0:valid_size]\n",
    "valid_labels = valid_labels[0:valid_size]\n",
    "\n",
    "print(train_dataset.shape)\n",
    "\n",
    "num_examples = len(train_dataset) # training set size\n",
    "print(num_examples)\n",
    "nn_input_dim = 784 # input layer dimensionality\n",
    "nn_hidden_dim_1 = [100,200,400]\n",
    "nn_hidden_dim_2 = [20,30,50]\n",
    "nn_output_dim = 10 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = [0.1,0.01,0.001] # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(dim_in, dim_h1, dim_h2, dim_out):\n",
    "    W1 = np.random.randn(dim_h1,dim_in)\n",
    "    b1 = np.zeros((dim_h1,1))\n",
    "    W2 = np.random.randn(dim_h2,dim_h1)\n",
    "    b2 = np.zeros((dim_h2,1))\n",
    "    W3 = np.random.randn(dim_out,dim_h2)\n",
    "    b3 = np.zeros((dim_out,1))\n",
    "    \n",
    "    return W1,b1,W2,b2,W3,b3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X,Y,dim_h1,dim_h2,learing_rate,reg_lambda,epoch=10000, print_cost = False):\n",
    "    decay_rate = 0.9\n",
    "    dim_in = X.shape[1]\n",
    "    dim_out = Y.shape[1]\n",
    "    m = X.shape[0]\n",
    "    cost_prev = 1000000\n",
    "    \n",
    "    W1 = np.random.randn(dim_in,dim_h1)\n",
    "    b1 = np.zeros((1,dim_h1))\n",
    "    W2 = np.random.randn(dim_h1,dim_h2)\n",
    "    b2 = np.zeros((1,dim_h2))\n",
    "    W3 = np.random.randn(dim_h2,dim_out)\n",
    "    b3 = np.zeros((1,dim_out))\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        #feed forward\n",
    "        Z1 = np.dot(X,W1)+b1\n",
    "        A1 = np.tanh(Z1)\n",
    "        Z2 = np.dot(A1,W2)+b2\n",
    "        A2 = np.tanh(Z2)\n",
    "        Z3 = np.dot(A2,W3)+b3\n",
    "        A3 = sigmoid(Z3)\n",
    "\n",
    "        #back propagation\n",
    "        dZ3 = A3-Y\n",
    "        dW3 = np.dot(A2.T,dZ3)*(1/m) + 2*reg_lambda*W3\n",
    "        db3 = np.sum(dZ3,axis=0)\n",
    "        dZ2 = np.dot(dZ3,W3.T)*(1-np.power(A2,2))\n",
    "        dW2 = np.dot(A1.T,dZ2)*(1/m) + 2*reg_lambda*W2\n",
    "        db2 = np.sum(dZ2,axis=0)\n",
    "        dZ1 = np.dot(dZ2,W2.T)*(1-np.power(A1,2))\n",
    "        dW1 = np.dot(X.T,dZ1)*(1/m) + 2*reg_lambda*W1\n",
    "        db1 = np.sum(dZ1,axis=0)\n",
    "\n",
    "        #update parameter\n",
    "        W1 = W1 - learing_rate * dW1\n",
    "        W2 = W2 - learing_rate * dW2\n",
    "        W3 = W3 - learing_rate * dW3\n",
    "        b1 = b1 - learing_rate * db1\n",
    "        b2 = b2 - learing_rate * db2\n",
    "        b3 = b3 - learing_rate * db3\n",
    "        \n",
    "        #calculate cost\n",
    "        cross_entropy = -np.sum(np.log(A3)*Y, axis=1)\n",
    "        cost = np.sum(cross_entropy)*(1/m)\n",
    "        \n",
    "        \n",
    "        if print_cost:\n",
    "            if(i%1000==999):\n",
    "                # learning rate decay\n",
    "                learing_rate = learing_rate * decay_rate\n",
    "                \n",
    "                if cost > cost_prev:\n",
    "                    learing_rate = learing_rate * 0.5\n",
    "                cost_prev = cost\n",
    "                \n",
    "                print(\"iteration {} cost : {}\".format(i+1,cost))\n",
    "        \n",
    "    \n",
    "    return {\n",
    "        'W1':W1,\n",
    "        'b1':b1,\n",
    "        'W2':W2,\n",
    "        'b2':b2,\n",
    "        'W3':W3,\n",
    "        'b3':b3,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,X,Y):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    W1 = model['W1']\n",
    "    b1 = model['b1']\n",
    "    W2 = model['W2']\n",
    "    b2 = model['b2']\n",
    "    W3 = model['W3']\n",
    "    b3 = model['b3']\n",
    "    \n",
    "    #feed forward\n",
    "    Z1 = np.dot(X,W1)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(A1,W2)+b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    Z3 = np.dot(A2,W3)+b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    pred = np.argmax(A3,axis=1)\n",
    "    target = np.argmax(Y,axis=1)\n",
    "    \n",
    "    cross_entropy = -np.sum(np.log(A3)*Y, axis=1)\n",
    "    cost = np.sum(cross_entropy)*(1/m)\n",
    "    \n",
    "    accuracy = sum(pred==target)/m\n",
    "    \n",
    "    return accuracy, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets select hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 cost : 82.8795162311825\n",
      "iteration 2000 cost : 84.77009893295944\n",
      "iteration 3000 cost : 81.91881530843597\n",
      "iteration 4000 cost : 86.29231073966056\n",
      "iteration 5000 cost : 101.9685157760117\n",
      "iteration 6000 cost : 69.90171798659294\n",
      "iteration 7000 cost : 79.51377600214808\n",
      "iteration 8000 cost : 78.33311319299264\n",
      "iteration 9000 cost : 57.70926585093017\n",
      "iteration 10000 cost : 62.22223898551865\n",
      "dim_h1 100 dim_h2 20 epsilon 0.1 : cost = 63.98430281805804, accuracy = 0.105\n",
      "iteration 1000 cost : 98.3447158398883\n",
      "iteration 2000 cost : 79.63912740690466\n",
      "iteration 3000 cost : 91.47581073068604\n",
      "iteration 4000 cost : 114.22744667184135\n",
      "iteration 5000 cost : 50.690413021792004\n",
      "iteration 6000 cost : 72.80719326041816\n",
      "iteration 7000 cost : 92.84369099033243\n",
      "iteration 8000 cost : 77.34644692812726\n",
      "iteration 9000 cost : 83.21733421416788\n",
      "iteration 10000 cost : 90.15053783932865\n",
      "dim_h1 100 dim_h2 30 epsilon 0.1 : cost = 85.25227355883602, accuracy = 0.125\n",
      "iteration 1000 cost : 72.70597563709866\n",
      "iteration 2000 cost : 106.27066658891742\n",
      "iteration 3000 cost : 78.05805569811133\n",
      "iteration 4000 cost : 57.96420093292104\n",
      "iteration 5000 cost : 74.13509465844348\n",
      "iteration 6000 cost : 98.54025790708017\n",
      "iteration 7000 cost : 71.32397777426252\n",
      "iteration 8000 cost : 62.8105067235872\n",
      "iteration 9000 cost : 70.00231997391171\n",
      "iteration 10000 cost : 63.79815840549437\n",
      "dim_h1 100 dim_h2 50 epsilon 0.1 : cost = 68.95239918821574, accuracy = 0.105\n",
      "iteration 1000 cost : 87.94926403762736\n",
      "iteration 2000 cost : 85.38704754529775\n",
      "iteration 3000 cost : 95.48156445643437\n",
      "iteration 4000 cost : 115.22959740807158\n",
      "iteration 5000 cost : 80.54678236800481\n",
      "iteration 6000 cost : 99.09445539554872\n",
      "iteration 7000 cost : 115.01057591495017\n",
      "iteration 8000 cost : 87.65000500185418\n",
      "iteration 9000 cost : 82.50676084966923\n",
      "iteration 10000 cost : 81.21454037534183\n",
      "dim_h1 200 dim_h2 20 epsilon 0.1 : cost = 98.89617930782403, accuracy = 0.12\n",
      "iteration 1000 cost : 53.446235526031685\n",
      "iteration 2000 cost : 50.27267216722063\n",
      "iteration 3000 cost : 64.59245309239407\n",
      "iteration 4000 cost : 93.5076656741841\n",
      "iteration 5000 cost : 83.65967586036274\n",
      "iteration 6000 cost : 106.28361819081154\n",
      "iteration 7000 cost : 88.99943748496709\n",
      "iteration 8000 cost : 86.31411595466687\n",
      "iteration 9000 cost : 67.80879297834407\n",
      "iteration 10000 cost : 57.94592213026877\n",
      "dim_h1 200 dim_h2 30 epsilon 0.1 : cost = 60.084549373151845, accuracy = 0.105\n",
      "iteration 1000 cost : 104.69833193409467\n",
      "iteration 2000 cost : 76.64975964868037\n",
      "iteration 3000 cost : 86.21495631448416\n",
      "iteration 4000 cost : 105.27850306896606\n",
      "iteration 5000 cost : 116.80989799583176\n",
      "iteration 6000 cost : 88.33664555795308\n",
      "iteration 7000 cost : 82.34272429347193\n",
      "iteration 8000 cost : 70.46130205268176\n",
      "iteration 9000 cost : 62.77781733043201\n",
      "iteration 10000 cost : 82.96202850211965\n",
      "dim_h1 200 dim_h2 50 epsilon 0.1 : cost = 87.05087931102732, accuracy = 0.105\n",
      "iteration 1000 cost : 96.94782945868087\n",
      "iteration 2000 cost : 82.32254396756296\n",
      "iteration 3000 cost : 41.553603005174196\n",
      "iteration 4000 cost : 39.06070533577508\n",
      "iteration 5000 cost : 69.90130699052574\n",
      "iteration 6000 cost : 73.98742708979064\n",
      "iteration 7000 cost : 112.66534674846604\n",
      "iteration 8000 cost : 109.04649958467265\n",
      "iteration 9000 cost : 87.57218382096411\n",
      "iteration 10000 cost : 85.38797247727294\n",
      "dim_h1 400 dim_h2 20 epsilon 0.1 : cost = 86.19108455482099, accuracy = 0.12\n",
      "iteration 1000 cost : 69.07982887571491\n",
      "iteration 2000 cost : 99.09358958913968\n",
      "iteration 3000 cost : 100.73296425600161\n",
      "iteration 4000 cost : 68.69153621065232\n",
      "iteration 5000 cost : 73.9566729068962\n",
      "iteration 6000 cost : 88.12033210679309\n",
      "iteration 7000 cost : 89.09784136164771\n",
      "iteration 8000 cost : 80.58484994794007\n",
      "iteration 9000 cost : 57.868777168664046\n",
      "iteration 10000 cost : 76.84176296669408\n",
      "dim_h1 400 dim_h2 30 epsilon 0.1 : cost = 77.64363898518995, accuracy = 0.12\n",
      "iteration 1000 cost : 38.677619108052625\n",
      "iteration 2000 cost : 70.36955577998259\n",
      "iteration 3000 cost : 83.96858239247065\n",
      "iteration 4000 cost : 95.23063743601602\n",
      "iteration 5000 cost : 91.38303707634525\n",
      "iteration 6000 cost : 89.92571772192434\n",
      "iteration 7000 cost : 111.718708526128\n",
      "iteration 8000 cost : 103.31221493962067\n",
      "iteration 9000 cost : 60.82369933558369\n",
      "iteration 10000 cost : 47.16549113700167\n",
      "dim_h1 400 dim_h2 50 epsilon 0.1 : cost = 53.335663288557356, accuracy = 0.105\n",
      "iteration 1000 cost : 1.9878667441373319\n",
      "iteration 2000 cost : 1.7904370231018378\n",
      "iteration 3000 cost : 1.6195398291828993\n",
      "iteration 4000 cost : 1.4430951566319143\n",
      "iteration 5000 cost : 1.3138445964255723\n",
      "iteration 6000 cost : 1.173364945701572\n",
      "iteration 7000 cost : 1.1652381070532454\n",
      "iteration 8000 cost : 0.9897214503713669\n",
      "iteration 9000 cost : 1.0554933239479973\n",
      "iteration 10000 cost : 0.8683619253414723\n",
      "dim_h1 100 dim_h2 20 epsilon 0.01 : cost = 1.0651790889195616, accuracy = 0.73\n",
      "iteration 1000 cost : 1.8846727366639324\n",
      "iteration 2000 cost : 1.623557437629932\n",
      "iteration 3000 cost : 1.4195281985907486\n",
      "iteration 4000 cost : 1.194937432080598\n",
      "iteration 5000 cost : 1.024314519195857\n",
      "iteration 6000 cost : 0.905124166103197\n",
      "iteration 7000 cost : 0.8565926884668477\n",
      "iteration 8000 cost : 0.7646120602582502\n",
      "iteration 9000 cost : 0.7208344297267493\n",
      "iteration 10000 cost : 0.6553324966869889\n",
      "dim_h1 100 dim_h2 30 epsilon 0.01 : cost = 0.972719361968512, accuracy = 0.795\n",
      "iteration 1000 cost : 1.6337064300541793\n",
      "iteration 2000 cost : 1.2658840984168622\n",
      "iteration 3000 cost : 1.0774970763697227\n",
      "iteration 4000 cost : 0.9425356095433115\n",
      "iteration 5000 cost : 0.8283215969082283\n",
      "iteration 6000 cost : 0.7307411390836415\n",
      "iteration 7000 cost : 0.6557901680369123\n",
      "iteration 8000 cost : 0.6690505353617614\n",
      "iteration 9000 cost : 0.6145445707732693\n",
      "iteration 10000 cost : 0.5947726704834694\n",
      "dim_h1 100 dim_h2 50 epsilon 0.01 : cost = 0.9142958465107188, accuracy = 0.765\n",
      "iteration 1000 cost : 2.00523886168745\n",
      "iteration 2000 cost : 1.6526241603486178\n",
      "iteration 3000 cost : 1.421757263209808\n",
      "iteration 4000 cost : 1.1987361104569458\n",
      "iteration 5000 cost : 0.9996456715036104\n",
      "iteration 6000 cost : 0.8443168636233881\n",
      "iteration 7000 cost : 0.7614886555492869\n",
      "iteration 8000 cost : 0.7375689900559865\n",
      "iteration 9000 cost : 0.6987236201787425\n",
      "iteration 10000 cost : 0.7957084364663621\n",
      "dim_h1 200 dim_h2 20 epsilon 0.01 : cost = 1.0484653209704997, accuracy = 0.675\n",
      "iteration 1000 cost : 1.8453315488543685\n",
      "iteration 2000 cost : 1.438802140807644\n",
      "iteration 3000 cost : 1.2290665041106141\n",
      "iteration 4000 cost : 1.0760558247469194\n",
      "iteration 5000 cost : 0.9239898469165125\n",
      "iteration 6000 cost : 0.7858089995464754\n",
      "iteration 7000 cost : 0.6939522030876877\n",
      "iteration 8000 cost : 0.626610886260521\n",
      "iteration 9000 cost : 0.6105969607011847\n",
      "iteration 10000 cost : 0.5972236911823862\n",
      "dim_h1 200 dim_h2 30 epsilon 0.01 : cost = 0.945731634551558, accuracy = 0.77\n",
      "iteration 1000 cost : 1.8304816432804563\n",
      "iteration 2000 cost : 1.252862411280144\n",
      "iteration 3000 cost : 0.995891607435609\n",
      "iteration 4000 cost : 0.8376545691716117\n",
      "iteration 5000 cost : 0.7051814136058292\n",
      "iteration 6000 cost : 0.6069071226288767\n",
      "iteration 7000 cost : 0.5430834600597694\n",
      "iteration 8000 cost : 0.4967798142986491\n",
      "iteration 9000 cost : 0.5098068296704268\n",
      "iteration 10000 cost : 0.47866780605540316\n",
      "dim_h1 200 dim_h2 50 epsilon 0.01 : cost = 0.9580907906416458, accuracy = 0.765\n",
      "iteration 1000 cost : 1.810840535567093\n",
      "iteration 2000 cost : 1.3574457438328853\n",
      "iteration 3000 cost : 1.128994446397533\n",
      "iteration 4000 cost : 0.9765325914538123\n",
      "iteration 5000 cost : 0.8153241549231113\n",
      "iteration 6000 cost : 0.68642277569841\n",
      "iteration 7000 cost : 0.5928423358163697\n",
      "iteration 8000 cost : 0.530218948346656\n",
      "iteration 9000 cost : 0.4868880992093857\n",
      "iteration 10000 cost : 0.4765299057716751\n",
      "dim_h1 400 dim_h2 20 epsilon 0.01 : cost = 0.9087011291997799, accuracy = 0.785\n",
      "iteration 1000 cost : 1.821934814768332\n",
      "iteration 2000 cost : 1.2564238493896873\n",
      "iteration 3000 cost : 1.0095126752820716\n",
      "iteration 4000 cost : 0.8550301585400385\n",
      "iteration 5000 cost : 0.7273348290420016\n",
      "iteration 6000 cost : 0.6162910254064057\n",
      "iteration 7000 cost : 0.5225293526427347\n",
      "iteration 8000 cost : 0.4532943816255616\n",
      "iteration 9000 cost : 0.4026424582504493\n",
      "iteration 10000 cost : 0.4309242382835598\n",
      "dim_h1 400 dim_h2 30 epsilon 0.01 : cost = 1.0077078680676268, accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 cost : 1.7080091840951543\n",
      "iteration 2000 cost : 1.1068143578303835\n",
      "iteration 3000 cost : 0.8601663068488712\n",
      "iteration 4000 cost : 0.7018675113058449\n",
      "iteration 5000 cost : 0.5935016171124449\n",
      "iteration 6000 cost : 0.5189593748295716\n",
      "iteration 7000 cost : 0.4573810144313781\n",
      "iteration 8000 cost : 0.4126100349030779\n",
      "iteration 9000 cost : 0.3789598374041059\n",
      "iteration 10000 cost : 0.39515705450539484\n",
      "dim_h1 400 dim_h2 50 epsilon 0.01 : cost = 0.8806560541180447, accuracy = 0.795\n",
      "iteration 1000 cost : 2.481651024795913\n",
      "iteration 2000 cost : 2.3296928754039254\n",
      "iteration 3000 cost : 2.2555673308259077\n",
      "iteration 4000 cost : 2.2106131996249725\n",
      "iteration 5000 cost : 2.183734987862936\n",
      "iteration 6000 cost : 2.1546656630921652\n",
      "iteration 7000 cost : 2.134726106092771\n",
      "iteration 8000 cost : 2.116967469463713\n",
      "iteration 9000 cost : 2.0981469597651805\n",
      "iteration 10000 cost : 2.0794257163338217\n",
      "dim_h1 100 dim_h2 20 epsilon 0.001 : cost = 2.2343523417264404, accuracy = 0.185\n",
      "iteration 1000 cost : 2.7852961375559073\n",
      "iteration 2000 cost : 2.286260499731603\n",
      "iteration 3000 cost : 2.11458978530953\n",
      "iteration 4000 cost : 2.0135654625501287\n",
      "iteration 5000 cost : 1.937508759506379\n",
      "iteration 6000 cost : 1.8760173173167711\n",
      "iteration 7000 cost : 1.8221869979733\n",
      "iteration 8000 cost : 1.7738490000773777\n",
      "iteration 9000 cost : 1.7305253278312285\n",
      "iteration 10000 cost : 1.6901147027352204\n",
      "dim_h1 100 dim_h2 30 epsilon 0.001 : cost = 2.0197494984094293, accuracy = 0.38\n",
      "iteration 1000 cost : 3.57711859816177\n",
      "iteration 2000 cost : 3.021021508973875\n",
      "iteration 3000 cost : 2.712608598820697\n",
      "iteration 4000 cost : 2.4818423239592677\n",
      "iteration 5000 cost : 2.3046880204184474\n",
      "iteration 6000 cost : 2.158194507204311\n",
      "iteration 7000 cost : 2.037619094999729\n",
      "iteration 8000 cost : 1.9322922015763142\n",
      "iteration 9000 cost : 1.8354351922132672\n",
      "iteration 10000 cost : 1.748962885964007\n",
      "dim_h1 100 dim_h2 50 epsilon 0.001 : cost = 2.475251864616041, accuracy = 0.315\n",
      "iteration 1000 cost : 3.0526167693086084\n",
      "iteration 2000 cost : 2.6486694337837386\n",
      "iteration 3000 cost : 2.4557669745276782\n",
      "iteration 4000 cost : 2.306048038440448\n",
      "iteration 5000 cost : 2.1787550247881455\n",
      "iteration 6000 cost : 2.078872499156808\n",
      "iteration 7000 cost : 1.9910190128467744\n",
      "iteration 8000 cost : 1.9123086619941845\n",
      "iteration 9000 cost : 1.845938080066752\n",
      "iteration 10000 cost : 1.7853227718045597\n",
      "dim_h1 200 dim_h2 20 epsilon 0.001 : cost = 2.000825849055198, accuracy = 0.335\n",
      "iteration 1000 cost : 3.3714476732323457\n",
      "iteration 2000 cost : 2.738067102189938\n",
      "iteration 3000 cost : 2.442558374707318\n",
      "iteration 4000 cost : 2.2428604404534567\n",
      "iteration 5000 cost : 2.0964443976913625\n",
      "iteration 6000 cost : 1.978253865069435\n",
      "iteration 7000 cost : 1.8794592517298667\n",
      "iteration 8000 cost : 1.7988089201214676\n",
      "iteration 9000 cost : 1.7275731857539487\n",
      "iteration 10000 cost : 1.663522678664855\n",
      "dim_h1 200 dim_h2 30 epsilon 0.001 : cost = 2.361644553929918, accuracy = 0.33\n",
      "iteration 1000 cost : 4.4222828931952085\n",
      "iteration 2000 cost : 3.6937217880373585\n",
      "iteration 3000 cost : 3.221590693070089\n",
      "iteration 4000 cost : 2.863670358382778\n",
      "iteration 5000 cost : 2.5915197831767776\n",
      "iteration 6000 cost : 2.3758406681373088\n",
      "iteration 7000 cost : 2.2013838747117735\n",
      "iteration 8000 cost : 2.0546638591867623\n",
      "iteration 9000 cost : 1.9263395582200835\n",
      "iteration 10000 cost : 1.816018289499587\n",
      "dim_h1 200 dim_h2 50 epsilon 0.001 : cost = 2.6809639249693418, accuracy = 0.385\n",
      "iteration 1000 cost : 3.984795279223753\n",
      "iteration 2000 cost : 3.458911363747437\n",
      "iteration 3000 cost : 3.071412168392275\n",
      "iteration 4000 cost : 2.802205684828303\n",
      "iteration 5000 cost : 2.594466969684456\n",
      "iteration 6000 cost : 2.4185167014378997\n",
      "iteration 7000 cost : 2.2761472094138044\n",
      "iteration 8000 cost : 2.1549773730910706\n",
      "iteration 9000 cost : 2.0436702610315702\n",
      "iteration 10000 cost : 1.9518330400210835\n",
      "dim_h1 400 dim_h2 20 epsilon 0.001 : cost = 2.7496461177529596, accuracy = 0.265\n",
      "iteration 1000 cost : 3.9696341424848236\n",
      "iteration 2000 cost : 3.4499119955413673\n",
      "iteration 3000 cost : 3.0858605640271763\n",
      "iteration 4000 cost : 2.8098327014399893\n",
      "iteration 5000 cost : 2.5848670554471727\n",
      "iteration 6000 cost : 2.408789691084037\n",
      "iteration 7000 cost : 2.2676689620147963\n",
      "iteration 8000 cost : 2.140695337894211\n",
      "iteration 9000 cost : 2.016508477468663\n",
      "iteration 10000 cost : 1.9162955950536733\n",
      "dim_h1 400 dim_h2 30 epsilon 0.001 : cost = 2.7381162298219035, accuracy = 0.33\n",
      "iteration 1000 cost : 4.674105973244945\n",
      "iteration 2000 cost : 3.823738842940621\n",
      "iteration 3000 cost : 3.2959580129553023\n",
      "iteration 4000 cost : 2.9187387276887766\n",
      "iteration 5000 cost : 2.6347140628780568\n",
      "iteration 6000 cost : 2.403469888998567\n",
      "iteration 7000 cost : 2.217892742830972\n",
      "iteration 8000 cost : 2.056598238926839\n",
      "iteration 9000 cost : 1.9229375401698847\n",
      "iteration 10000 cost : 1.808011028768139\n",
      "dim_h1 400 dim_h2 50 epsilon 0.001 : cost = 3.037029000418729, accuracy = 0.31\n"
     ]
    }
   ],
   "source": [
    "for e in epsilon:\n",
    "    for hid_1 in nn_hidden_dim_1:\n",
    "        for hid_2 in nn_hidden_dim_2:\n",
    "            model = build_model(train_dataset, train_labels, hid_1, hid_2, e, reg_lambda,print_cost= True)\n",
    "            acc, cost = test(model, valid_dataset, valid_labels)\n",
    "            print(\"dim_h1 {} dim_h2 {} epsilon {} : cost = {}, accuracy = {}\".format(hid_1, hid_2,e, cost,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning rate = 0.01 is optimal, hidden layer test again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_h1 50 dim_h2 20 epsilon 0.01 : cost = 1.2700413144452758, accuracy = 0.685\n",
      "dim_h1 50 dim_h2 30 epsilon 0.01 : cost = 1.0020496725565586, accuracy = 0.73\n",
      "dim_h1 50 dim_h2 50 epsilon 0.01 : cost = 0.9814453805316734, accuracy = 0.75\n",
      "dim_h1 50 dim_h2 70 epsilon 0.01 : cost = 1.003839748035584, accuracy = 0.73\n",
      "dim_h1 50 dim_h2 100 epsilon 0.01 : cost = 1.0523197469344014, accuracy = 0.745\n",
      "dim_h1 50 dim_h2 150 epsilon 0.01 : cost = 0.9380602843021668, accuracy = 0.76\n",
      "dim_h1 50 dim_h2 200 epsilon 0.01 : cost = 0.9183202295245277, accuracy = 0.755\n",
      "dim_h1 50 dim_h2 250 epsilon 0.01 : cost = 1.0580843142112455, accuracy = 0.735\n",
      "dim_h1 50 dim_h2 300 epsilon 0.01 : cost = 0.8772264317188715, accuracy = 0.79\n",
      "dim_h1 50 dim_h2 400 epsilon 0.01 : cost = 0.9233949111314874, accuracy = 0.775\n",
      "dim_h1 100 dim_h2 20 epsilon 0.01 : cost = 1.0451622994133636, accuracy = 0.77\n",
      "dim_h1 100 dim_h2 30 epsilon 0.01 : cost = 1.0828539782026574, accuracy = 0.7\n",
      "dim_h1 100 dim_h2 50 epsilon 0.01 : cost = 1.0186657556289416, accuracy = 0.74\n",
      "dim_h1 100 dim_h2 70 epsilon 0.01 : cost = 0.9144676419079467, accuracy = 0.77\n",
      "dim_h1 100 dim_h2 100 epsilon 0.01 : cost = 0.9327272103544664, accuracy = 0.77\n",
      "dim_h1 100 dim_h2 150 epsilon 0.01 : cost = 0.9317177036944051, accuracy = 0.75\n",
      "dim_h1 100 dim_h2 200 epsilon 0.01 : cost = 0.9415287157936493, accuracy = 0.755\n",
      "dim_h1 100 dim_h2 250 epsilon 0.01 : cost = 1.0168132921886808, accuracy = 0.765\n",
      "dim_h1 100 dim_h2 300 epsilon 0.01 : cost = 0.8681395782098008, accuracy = 0.755\n",
      "dim_h1 100 dim_h2 400 epsilon 0.01 : cost = 1.0057816538554918, accuracy = 0.765\n",
      "dim_h1 150 dim_h2 20 epsilon 0.01 : cost = 1.1218768751518355, accuracy = 0.71\n",
      "dim_h1 150 dim_h2 30 epsilon 0.01 : cost = 0.894874694741483, accuracy = 0.775\n",
      "dim_h1 150 dim_h2 50 epsilon 0.01 : cost = 0.9110602950665072, accuracy = 0.79\n",
      "dim_h1 150 dim_h2 70 epsilon 0.01 : cost = 0.8928903225592998, accuracy = 0.795\n",
      "dim_h1 150 dim_h2 100 epsilon 0.01 : cost = 0.881239064968394, accuracy = 0.77\n",
      "dim_h1 150 dim_h2 150 epsilon 0.01 : cost = 0.9555408474245113, accuracy = 0.77\n",
      "dim_h1 150 dim_h2 200 epsilon 0.01 : cost = 0.924934058016703, accuracy = 0.735\n",
      "dim_h1 150 dim_h2 250 epsilon 0.01 : cost = 0.984025625914705, accuracy = 0.77\n",
      "dim_h1 150 dim_h2 300 epsilon 0.01 : cost = 0.9429160716703856, accuracy = 0.76\n",
      "dim_h1 150 dim_h2 400 epsilon 0.01 : cost = 1.0363695162088278, accuracy = 0.755\n",
      "dim_h1 200 dim_h2 20 epsilon 0.01 : cost = 0.9969767784947832, accuracy = 0.73\n",
      "dim_h1 200 dim_h2 30 epsilon 0.01 : cost = 1.0601425253208745, accuracy = 0.715\n",
      "dim_h1 200 dim_h2 50 epsilon 0.01 : cost = 0.8804371608074004, accuracy = 0.765\n",
      "dim_h1 200 dim_h2 70 epsilon 0.01 : cost = 0.9005070747705054, accuracy = 0.795\n",
      "dim_h1 200 dim_h2 100 epsilon 0.01 : cost = 0.870547606868954, accuracy = 0.76\n",
      "dim_h1 200 dim_h2 150 epsilon 0.01 : cost = 0.9108466612039454, accuracy = 0.74\n",
      "dim_h1 200 dim_h2 200 epsilon 0.01 : cost = 0.945049708684651, accuracy = 0.75\n",
      "dim_h1 200 dim_h2 250 epsilon 0.01 : cost = 0.9078660243327008, accuracy = 0.79\n",
      "dim_h1 200 dim_h2 300 epsilon 0.01 : cost = 0.9789223469533073, accuracy = 0.77\n",
      "dim_h1 200 dim_h2 400 epsilon 0.01 : cost = 0.9959731540453697, accuracy = 0.76\n",
      "dim_h1 250 dim_h2 20 epsilon 0.01 : cost = 0.9763644898065124, accuracy = 0.775\n",
      "dim_h1 250 dim_h2 30 epsilon 0.01 : cost = 0.9817737828556274, accuracy = 0.735\n",
      "dim_h1 250 dim_h2 50 epsilon 0.01 : cost = 0.9492456344779474, accuracy = 0.77\n",
      "dim_h1 250 dim_h2 70 epsilon 0.01 : cost = 0.9742368731282679, accuracy = 0.77\n",
      "dim_h1 250 dim_h2 100 epsilon 0.01 : cost = 0.9278831386196721, accuracy = 0.765\n",
      "dim_h1 250 dim_h2 150 epsilon 0.01 : cost = 0.9547000169269716, accuracy = 0.78\n",
      "dim_h1 250 dim_h2 200 epsilon 0.01 : cost = 0.9381528163541368, accuracy = 0.735\n",
      "dim_h1 250 dim_h2 250 epsilon 0.01 : cost = 0.9065532893154494, accuracy = 0.795\n",
      "dim_h1 250 dim_h2 300 epsilon 0.01 : cost = 0.9782519580881551, accuracy = 0.755\n",
      "dim_h1 250 dim_h2 400 epsilon 0.01 : cost = 0.9379122568652435, accuracy = 0.8\n",
      "dim_h1 300 dim_h2 20 epsilon 0.01 : cost = 0.9970733818269, accuracy = 0.73\n",
      "dim_h1 300 dim_h2 30 epsilon 0.01 : cost = 0.8632890443743769, accuracy = 0.79\n",
      "dim_h1 300 dim_h2 50 epsilon 0.01 : cost = 0.8796602312907856, accuracy = 0.78\n",
      "dim_h1 300 dim_h2 70 epsilon 0.01 : cost = 0.9891592886725599, accuracy = 0.765\n",
      "dim_h1 300 dim_h2 100 epsilon 0.01 : cost = 0.9328976659880531, accuracy = 0.785\n",
      "dim_h1 300 dim_h2 150 epsilon 0.01 : cost = 1.0090943594470965, accuracy = 0.75\n",
      "dim_h1 300 dim_h2 200 epsilon 0.01 : cost = 0.9342476561969306, accuracy = 0.745\n",
      "dim_h1 300 dim_h2 250 epsilon 0.01 : cost = 0.9643653577706742, accuracy = 0.775\n",
      "dim_h1 300 dim_h2 300 epsilon 0.01 : cost = 1.0458362296036523, accuracy = 0.75\n",
      "dim_h1 300 dim_h2 400 epsilon 0.01 : cost = 1.0974621749897968, accuracy = 0.735\n",
      "dim_h1 350 dim_h2 20 epsilon 0.01 : cost = 0.9720339459058803, accuracy = 0.75\n",
      "dim_h1 350 dim_h2 30 epsilon 0.01 : cost = 0.911043248359543, accuracy = 0.77\n",
      "dim_h1 350 dim_h2 50 epsilon 0.01 : cost = 0.9453699226135872, accuracy = 0.775\n",
      "dim_h1 350 dim_h2 70 epsilon 0.01 : cost = 0.9930307337676566, accuracy = 0.745\n",
      "dim_h1 350 dim_h2 100 epsilon 0.01 : cost = 0.8820151094154522, accuracy = 0.785\n",
      "dim_h1 350 dim_h2 150 epsilon 0.01 : cost = 0.9477200261504853, accuracy = 0.77\n",
      "dim_h1 350 dim_h2 200 epsilon 0.01 : cost = 1.0307801540256352, accuracy = 0.745\n",
      "dim_h1 350 dim_h2 250 epsilon 0.01 : cost = 1.0236772973658597, accuracy = 0.755\n",
      "dim_h1 350 dim_h2 300 epsilon 0.01 : cost = 0.9610747540801726, accuracy = 0.775\n",
      "dim_h1 350 dim_h2 400 epsilon 0.01 : cost = 1.236821913462236, accuracy = 0.725\n",
      "dim_h1 400 dim_h2 20 epsilon 0.01 : cost = 0.9286795846948878, accuracy = 0.75\n",
      "dim_h1 400 dim_h2 30 epsilon 0.01 : cost = 0.8604195973094931, accuracy = 0.8\n",
      "dim_h1 400 dim_h2 50 epsilon 0.01 : cost = 0.9488744172242468, accuracy = 0.765\n",
      "dim_h1 400 dim_h2 70 epsilon 0.01 : cost = 0.9244202781452958, accuracy = 0.775\n",
      "dim_h1 400 dim_h2 100 epsilon 0.01 : cost = 0.9473027625322373, accuracy = 0.785\n",
      "dim_h1 400 dim_h2 150 epsilon 0.01 : cost = 0.9734377732296591, accuracy = 0.75\n",
      "dim_h1 400 dim_h2 200 epsilon 0.01 : cost = 0.8917549141712369, accuracy = 0.765\n",
      "dim_h1 400 dim_h2 250 epsilon 0.01 : cost = 0.9513870880389322, accuracy = 0.78\n",
      "dim_h1 400 dim_h2 300 epsilon 0.01 : cost = 0.9458201278841338, accuracy = 0.78\n",
      "dim_h1 400 dim_h2 400 epsilon 0.01 : cost = 1.1525085331019238, accuracy = 0.73\n",
      "dim_h1 450 dim_h2 20 epsilon 0.01 : cost = 0.9315303433234232, accuracy = 0.765\n",
      "dim_h1 450 dim_h2 30 epsilon 0.01 : cost = 0.9308088191060567, accuracy = 0.775\n",
      "dim_h1 450 dim_h2 50 epsilon 0.01 : cost = 0.91230915645899, accuracy = 0.76\n",
      "dim_h1 450 dim_h2 70 epsilon 0.01 : cost = 0.8852364129748651, accuracy = 0.795\n",
      "dim_h1 450 dim_h2 100 epsilon 0.01 : cost = 0.9764190143088466, accuracy = 0.76\n",
      "dim_h1 450 dim_h2 150 epsilon 0.01 : cost = 1.0011320481051984, accuracy = 0.745\n",
      "dim_h1 450 dim_h2 200 epsilon 0.01 : cost = 1.0421856850851754, accuracy = 0.715\n",
      "dim_h1 450 dim_h2 250 epsilon 0.01 : cost = 0.905629192113219, accuracy = 0.795\n",
      "dim_h1 450 dim_h2 300 epsilon 0.01 : cost = 0.8887575691771641, accuracy = 0.81\n",
      "dim_h1 450 dim_h2 400 epsilon 0.01 : cost = 1.0540945873344163, accuracy = 0.765\n",
      "dim_h1 500 dim_h2 20 epsilon 0.01 : cost = 0.9817604818857741, accuracy = 0.745\n",
      "dim_h1 500 dim_h2 30 epsilon 0.01 : cost = 0.9016810388584935, accuracy = 0.775\n",
      "dim_h1 500 dim_h2 50 epsilon 0.01 : cost = 0.9447055164232085, accuracy = 0.765\n",
      "dim_h1 500 dim_h2 70 epsilon 0.01 : cost = 0.9256178779694483, accuracy = 0.805\n",
      "dim_h1 500 dim_h2 100 epsilon 0.01 : cost = 0.9505393613662841, accuracy = 0.77\n",
      "dim_h1 500 dim_h2 150 epsilon 0.01 : cost = 0.8834057833433334, accuracy = 0.77\n",
      "dim_h1 500 dim_h2 200 epsilon 0.01 : cost = 0.9437171930938206, accuracy = 0.75\n",
      "dim_h1 500 dim_h2 250 epsilon 0.01 : cost = 0.8767107692947661, accuracy = 0.77\n",
      "dim_h1 500 dim_h2 300 epsilon 0.01 : cost = 1.0336525623625767, accuracy = 0.765\n",
      "dim_h1 500 dim_h2 400 epsilon 0.01 : cost = 1.0276422338116704, accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "e = 0.01\n",
    "nn_hidden_dim_1 = [50,100,150,200,250,300,350,400,450,500]\n",
    "nn_hidden_dim_2 = [20,30,50,70,100,150,200,250,300,400]\n",
    "for hid_1 in nn_hidden_dim_1:\n",
    "    for hid_2 in nn_hidden_dim_2:\n",
    "        model = build_model(train_dataset, train_labels, hid_1, hid_2, e, reg_lambda,print_cost= False)\n",
    "        acc, cost = test(model, valid_dataset, valid_labels)\n",
    "        print(\"dim_h1 {} dim_h2 {} epsilon {} : cost = {}, accuracy = {}\".format(hid_1, hid_2,e, cost,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden layer 1 = 450, layer 2 = 300 is the best\n",
    "\n",
    "And it's result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 cost : 0.5589362103262916\n",
      "iteration 2000 cost : 0.2857177892919409\n",
      "iteration 3000 cost : 0.2086103930926925\n",
      "iteration 4000 cost : 0.1685329694670511\n",
      "iteration 5000 cost : 0.1496999336544728\n",
      "iteration 6000 cost : 0.1426500674112879\n",
      "iteration 7000 cost : 0.1379729294085051\n",
      "iteration 8000 cost : 0.13636778321346316\n",
      "iteration 9000 cost : 0.1369857547402311\n",
      "iteration 10000 cost : 0.13841317815429113\n",
      "iteration 11000 cost : 0.14008645251534416\n",
      "iteration 12000 cost : 0.14198971561608006\n",
      "iteration 13000 cost : 0.1442127151470001\n",
      "iteration 14000 cost : 0.14657355730923627\n",
      "iteration 15000 cost : 0.14893428091656358\n",
      "iteration 16000 cost : 0.15120806403005027\n",
      "iteration 17000 cost : 0.15335015603053131\n",
      "iteration 18000 cost : 0.1553471620653516\n",
      "iteration 19000 cost : 0.15719996927746252\n",
      "iteration 20000 cost : 0.15891239120295506\n",
      "dim_h1 450 dim_h2 300 epsilon 0.01 : cost = 1.1139135370289857, accuracy = 0.73\n"
     ]
    }
   ],
   "source": [
    "e=0.01\n",
    "hid_1=450\n",
    "hid_2=300\n",
    "\n",
    "model = build_model(train_dataset, train_labels, hid_1, hid_2, e, reg_lambda,epoch=20000,print_cost= True)\n",
    "acc, cost = test(model, test_dataset, test_labels)\n",
    "print(\"dim_h1 {} dim_h2 {} epsilon {} : cost = {}, accuracy = {}\".format(hid_1, hid_2,e, cost,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data accuracy : 0.8425, cost : 0.6776996527210342\n"
     ]
    }
   ],
   "source": [
    "acc, cost = test(model, test_dataset, test_labels)\n",
    "print('test data accuracy : {}, cost : {}'.format(acc, cost))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
